{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tsv to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = '/home/mhiraga/CorpusData_New/Corpus_Data/Annotated_ALL_Shina_Hira_2/S163220F.txt.tsv'\n",
    "df = pd.read_csv('/home/mhiraga/CorpusData_New/Corpus_Data/Annotated_ALL_Shina_Hira_2/20216S04.txt.tsv', \n",
    "                     sep='\\t', skiprows=8, comment='#',header=None,skip_blank_lines=True,encoding='utf-8') #index_col=0 # to set the index to column 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features\n",
    "\n",
    "Using informative linguistic features can improve a machine learning model. I investigated which features were most effective for the automatic annotation using the corpus data. I used various types of features, including POS tags and lexical items in a sliding window around the target particle, which have been used by previous English preposition \\citep{tetreault2008ups} and Korean particle error detection systems \\citep{,RossIsraelDiss}. I also included dependency relations used by Japanese particle error detection systems \\citep{suzuki2006learning,oyama2016automatic}. \n",
    "I run non-exhaustive feature selection experiments on the development sets for two particle groups, {\\it de} and {\\it ni}. I chose these particle groups because the group {\\it de} has similar class distributions with other particles, and the {\\it ni} experiment contains relatively more distributed classes than other particles. Below is the list of features that are included in the feature selection experiments.\n",
    "\n",
    "Target Particle\n",
    "Word (window size -3 to +3)\n",
    "POS   (window size -3 to +3)\n",
    "Lemmas (window size -3 to +3)\n",
    "Category of the nouns selected in \\ref{featureword}\n",
    "Dependency head of the particle\n",
    "All the other particles in the same sentence\n",
    "All the verbs and auxiliary verbs that appear after the target particle\n",
    "\n",
    "The experiments were done manually by unselecting some of the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, glob\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "\n",
    "def getBunrui(word,POS):\n",
    "    \n",
    "    word_cat_N = set()\n",
    "    \n",
    "    if POS.startswith('名詞'):\n",
    "        try:\n",
    "            word_cat_N_list = bunrui[bunrui[12].str.match(r'^'+word+r'\\b')][5].to_numpy() \n",
    "            if len(word_cat_N_list) != 0:\n",
    "                word_cat_N_list[0] = word_cat_N_list[0].reset_index()\n",
    "                return word_cat_N_list[0]\n",
    "            else:\n",
    "                \n",
    "                return \"0\"\n",
    "        except:\n",
    "            return \"None\"\n",
    "\n",
    "    \n",
    "def labelReplace(wordlist): #replace words\n",
    "    wordlist = list(filter(('*').__ne__, wordlist))\n",
    "    wordlist = list(filter(('').__ne__, wordlist))\n",
    "    wordlist = list(filter(('_').__ne__, wordlist))\n",
    "    wordlist = list(filter(('-').__ne__, wordlist))\n",
    "    wordlist = [i for i in wordlist if not i in ['nan', np.nan]]\n",
    "    \n",
    "    for i in range(len(wordlist)):\n",
    "        #print(item)\n",
    "        \n",
    "        if str(wordlist[i]).startswith(\"Top\"):\n",
    "\n",
    "            wordlist[i] = 'Topic'\n",
    "\n",
    "        elif str(wordlist[i]) in ['NewSub', 'SubUnknown']:\n",
    "            wordlist[i] = 'Sub'\n",
    "        elif str(wordlist[i]) == 'Unkown':\n",
    "            wordlist[i] = 'Unknown'\n",
    "        elif str(wordlist[i]) == 'V-Mod':\n",
    "            wordlist[i] = 'VMod'\n",
    "        elif str(wordlist[i]) == 'Comp':\n",
    "            wordlist[i] = 'Quote/Comp'\n",
    "        elif str(wordlist[i])== 'Quote':\n",
    "            wordlist[i] = 'Quote/Comp'\n",
    "\n",
    "        elif str(wordlist[i]) == 'Obj-Ga':\n",
    "            wordlist[i] = 'Ga-Obj'\n",
    "        elif str(wordlist[i]) == 'NiObj':\n",
    "            wordlist[i] = 'NiID'\n",
    "        elif str(wordlist[i]) == 'NiNaru':\n",
    "            wordlist[i] = 'NiChange'\n",
    "    return wordlist\n",
    "\n",
    "def getNeighboringWords(filename, df, prt):  #get Neighboring words and their POS\n",
    "    \n",
    "    classlabel= []\n",
    "    correctprt = []\n",
    "    list_of_dict = []\n",
    "    feature_dict={}\n",
    "    target_word = {}\n",
    "    ## for addition error, there's no particles marked \"correct\". \n",
    "    ## Therefore, it is classification of finding which particle should be \"NULL\" or not, and if \"incorrect\", what function.\n",
    "    ## prt is a list of all particles here.\n",
    "    for index, row in df.iterrows():\n",
    "        feature_dict={}\n",
    "        \n",
    "        if (str(df.loc[index][5]) in [prt]) and str(df.loc[index][3]).split('\\\\')[0].replace('feature=','').startswith('助詞') and str(df.loc[index][6]) in ['Incorrect', 'Correct', 'Attn']:\n",
    "            if df.loc[index][6] == 'Correct':\n",
    "                \n",
    "                function_label = list(df.loc[index][9:20])\n",
    "                function_label = labelReplace(function_label)\n",
    "                function_label.append('COR')\n",
    "                \n",
    "                #classlabel.append(str(function_label)) \n",
    "                classlabel.append(function_label)\n",
    "            elif df.loc[index][6] in ['Incorrect','Attn']:\n",
    "                \n",
    "                function_label = list(df.loc[index][9:20])\n",
    "                function_label = labelReplace(function_label)\n",
    "                if len(function_label) > 0 and function_label[0] == \"Unknown\":\n",
    "                    if str(df.loc[index][7]).startswith(r\"[A-Za-z]\") == \"False\" or df.loc[index][7] == \"nan\" or pd.isna(df.loc[index][7]): # or df.loc[index][7] == np.nan :\n",
    "                        function_label.append(\"Other\")\n",
    "                    else:\n",
    "                        function_label.append(df.loc[index][7])\n",
    "                function_label.append('INC')\n",
    "                #classlabel.append(str(function_label)) # for multi class \n",
    "                classlabel.append(function_label)\n",
    "                \n",
    "            if df.loc[index][20] == 'D':\n",
    "                dephead_id = str(df.loc[index,22])\n",
    "                feature_dict['dep_head'] = df.loc[df[0]==dephead_id,5].values[0]\n",
    "                    \n",
    "                \n",
    "                    #print('22:',filename)\n",
    "            elif df.loc[index][22] == 'D':\n",
    "                dephead_id = str(df.loc[index,24])\n",
    "                feature_dict['dep_head'] = df.loc[df[0]==dephead_id,5].values[0]\n",
    "                    #print('24:',filename)\n",
    "           \n",
    "                \n",
    "            target_word[filename.replace('~/Documents/','')+'_'+str(df.loc[index][0])]=feature_dict\n",
    "            feature_dict['target']=str(df.loc[index][2])\n",
    "\n",
    "            \n",
    "            ## if target word is a particle, \n",
    "            ## get other particles in the sentence\n",
    "            ## otherwise, 'NONE'\n",
    "            target_word_id = str(df.loc[index][0])\n",
    "            same_sent_id = target_word_id.split('-')[0]\n",
    "\n",
    "            if (str(df.loc[index][5]) in prt) and str(df.loc[index][3]).split('\\\\')[0].replace('feature=','').startswith('助詞'):\n",
    "                \n",
    "                same_sentence = df.loc[df[0].str.contains('^'+same_sent_id+'-')==True]\n",
    "                \n",
    "                prt_list = []\n",
    "                verb_list = []\n",
    "                for i, row_i in same_sentence.iterrows():\n",
    "                    if index != i: \n",
    "                       \n",
    "                        if (str(df.loc[i][5]) in prt) and str(df.loc[i][3]).split('\\\\')[0].replace('feature=','').startswith('助詞'):\n",
    "                            prt_list.append(same_sentence.loc[i][5])\n",
    "                    #else:\n",
    "                    #    print(index, i)\n",
    "\n",
    "                    if str(same_sentence.loc[i][3]).split('\\\\')[0].replace('feature=','').startswith('動詞'):\n",
    "                        verb_list.append(same_sentence.loc[i][5]) # verb lemma\n",
    "                \n",
    "                for i in range(len(prt_list)):\n",
    "                    feature_dict['OtherPrt_'+str(i)] = prt_list[i]\n",
    "                \n",
    "                for j in range(len(verb_list)):\n",
    "                    feature_dict['Verbs_'+str(j)] = verb_list[j]\n",
    "                \n",
    "\n",
    "                \n",
    "            ##get dependency head\n",
    "            \n",
    "            if df.loc[index][20] == 'D':\n",
    "                dephead_id = str(df.loc[index,22])\n",
    "                feature_dict['dep_head'] = df.loc[df[0]==dephead_id,5].values[0]\n",
    "                \n",
    "            elif df.loc[index][22] == 'D':\n",
    "                dephead_id = str(df.loc[index,24])\n",
    "                feature_dict['dep_head'] = df.loc[df[0]==dephead_id,5].values[0]\n",
    "            \n",
    "            \n",
    "           #get words and POS in the sliding window -3 to +3\n",
    "            if index >= df.shape[0]-df.shape[0]+1:\n",
    "                word = str(df.loc[index-1][2])\n",
    "              \n",
    "                feature_dict['word-1']=str(df.loc[index-1][2])\n",
    "                feature_dict['lemma-1'] = str(df.loc[index-1][5])\n",
    "                POS = str(df.loc[index-1][3]).split('\\\\')[0].replace('feature=','').split(',')[0]\n",
    "                feature_dict['POS-1']=str(df.loc[index-1][3]).split('\\\\')[0].replace('feature=','').split(',')[0]\n",
    "                \n",
    "                #feature_dict['wordcat-1'] = getBunrui(word,POS) #this was not in the final version #this doesn't work\n",
    "                #getBunrui method didn't work, so below is code to get \"wordcat\" feature.\n",
    "                \n",
    "                if POS.startswith('名詞'):\n",
    "                    try:\n",
    "                        word_cat_N_list = bunrui[bunrui[12].str.match(r'^'+word+r'\\b')][5].to_numpy() #get_values() #r'^'+word+r'\\b'\n",
    "                        if len(word_cat_N_list) != 0:\n",
    "                            feature_dict['wordcat-1'] = word_cat_N_list[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                \n",
    "                    \n",
    "                    \n",
    "                        \n",
    "                if index >= df.shape[0]-df.shape[0]+2:\n",
    "                    \n",
    "                    word = str(df.loc[index-2][2]) \n",
    "                    feature_dict['word-2']=word \n",
    "                    #feature_dict['lemma-2'] = str(df.loc[index-2][5]) # not in the final version\n",
    "                    POS = str(df.loc[index-2][3]).split('\\\\')[0].replace('feature=','').split(',')[0]\n",
    "                    feature_dict['POS-2']=POS\n",
    "                    \n",
    "                    #feature_dict['wordcat-2'] = getBunrui(word,POS) # not in the final version\n",
    "                    \n",
    "                    if index >= df.shape[0]-df.shape[0]+3:\n",
    "                  \n",
    "                        feature_dict['word-3']=str(df.loc[index-3][2])\n",
    "                        #feature_dict['lemma-3'] = str(df.loc[index-3][5])\n",
    "                        feature_dict['POS-3']=str(df.loc[index-3][3]).split('\\\\')[0].replace('feature=','').split(',')[0]\n",
    "                   \n",
    "\n",
    "            if index < df.shape[0]-1:\n",
    "                word = str(df.loc[index+1][2])\n",
    "                feature_dict['word+1']=word\n",
    "                feature_dict['lemma+1'] = str(df.loc[index+1][5]) #not in the final\n",
    "                POS = str(df.loc[index+1][3]).split('\\\\')[0].replace('feature=','').split(',')[0]\n",
    "                feature_dict['POS+1']=POS\n",
    "                #feature_dict['wordcat+1'] = getBunrui(word,POS) #not in the final\n",
    "                \n",
    "                if index < df.shape[0]-2:\n",
    "                    pass\n",
    "                    feature_dict['word+2']=str(df.loc[index+2][2])\n",
    "                    #feature_dict['lemma+2'] = str(df.loc[index+2][5])\n",
    "                    feature_dict['POS+2']=str(df.loc[index+2][3]).split('\\\\')[0].replace('feature=','').split(',')[0]\n",
    "\n",
    "                    if index < df.shape[0]-3:\n",
    "          \n",
    "                        feature_dict['word+3']=str(df.loc[index+3][2])\n",
    "                        #feature_dict['lemma+3'] = str(df.loc[index+3][5])\n",
    "                        feature_dict['POS+3']=str(df.loc[index+3][3]).split('\\\\')[0].replace('feature=','').split(',')[0]\n",
    "               \n",
    "            list_of_dict.append(feature_dict)\n",
    "    \n",
    "  \n",
    "    return list_of_dict, classlabel #for multi_class classification\n",
    "    \n",
    "\n",
    "def put_together(path,prt):\n",
    "    everything = []\n",
    "    all_labels = []\n",
    "   \n",
    "    \n",
    "    files = [str(pp) for pp in glob.glob(path)]   \n",
    "  \n",
    "    for file in files:\n",
    "        \n",
    "        df = pd.read_csv(file,sep='\\t', skiprows=8, comment='#',header=None,keep_default_na=False, skip_blank_lines=True,encoding='utf-8') #index_col=0 # to set the index to column 0\n",
    "        \n",
    "        list_of_dict, labels = getNeighboringWords(file,df,prt)  \n",
    "        \n",
    "        everything += list_of_dict\n",
    "        all_labels += labels\n",
    "    \n",
    "    return everything, all_labels\n",
    "        \n",
    "def put_together_1(path,prt_list): #prt_list instead of prt\n",
    "    everything = []\n",
    "    all_labels = []\n",
    "  \n",
    "    \n",
    "    files = glob.glob(path)   \n",
    "    \n",
    "    for file in files:\n",
    "        df = pd.read_csv(file,sep='\\t',skiprows=8, comment='#',header=None,keep_default_na=False,skip_blank_lines=True,encoding='utf-8') #index_col=0 # to set the index to column 0\n",
    "        list_of_dict, labels = getNeighboringWords(file,df,prt_list)  \n",
    "        everything += list_of_dict\n",
    "        all_labels += labels\n",
    "    \n",
    "    return everything, all_labels        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train\n",
    "all_prt = ['に','で','を','は','が','と','も','の','NULL','Other']\n",
    "\n",
    "## running this will take a while\n",
    "path = '/home/mhiraga/CorpusData_New/Corpus_Data/experiment/train/*'\n",
    "\n",
    "everything_ni, all_labels_ni = put_together(path,'に')    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything_de, all_labels_de = put_together(path,'で')\n",
    "everything_o, all_labels_o = put_together(path,'を')\n",
    "everything_wa, all_labels_wa = put_together(path,'は')\n",
    "everything_ga, all_labels_ga = put_together(path,'が')\n",
    "everything_to, all_labels_to = put_together(path,'と')\n",
    "everything_mo, all_labels_mo = put_together(path,'も')\n",
    "everything_no, all_labels_no = put_together(path,'の')\n",
    "everything_other, all_labels_other = put_together(path,'Other')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "everything_null, all_labels_null = put_together(path,str('NULL'))\n",
    "print(len(everything_null))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2178 2178\n"
     ]
    }
   ],
   "source": [
    "def get_index_positions(list_of_elems, element):\n",
    "    ''' Returns the indexes of all occurrences of give element in\n",
    "    the list- listOfElements '''\n",
    "    index_pos_list = []\n",
    "    index_pos = 0\n",
    "    while True:\n",
    "        try:\n",
    "            # Search for item in list from indexPos to the end of list\n",
    "            index_pos = list_of_elems.index(element, index_pos)\n",
    "            # Add the index position in list\n",
    "            index_pos_list.append(index_pos)\n",
    "            index_pos += 1\n",
    "        except ValueError as e:\n",
    "            break\n",
    "    return index_pos_list\n",
    "\n",
    "print(len(everything_ni),len(all_labels_ni))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Synthetic Data from \"Correct\" samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def copied_together(everything,all_labels,all_prt, prt): #everything = original list of dict with labels, all_labels=class labels\n",
    "    \n",
    "    copied = {}\n",
    "    copied_labels = copy.deepcopy(all_labels)\n",
    "    \n",
    "    copied_labels_frame = pd.DataFrame({\"class\": copied_labels})\n",
    "    copied_all_labels = {}\n",
    "    inc_prtlist= [i for i in all_prt if i not in [prt]]\n",
    "\n",
    "    for i in inc_prtlist:\n",
    "        #print(i)\n",
    "        copied_df = pd.DataFrame(everything) # for each particle, create data frame from dict \"everything\"\n",
    "        copied_df = copied_df.join(copied_labels_frame)\n",
    "        \n",
    "       \n",
    "        \n",
    "        copied_df.loc[(copied_df[\"target\"] == prt),\"target\"] = i\n",
    "        \n",
    "        \n",
    "        for item in copied_df[\"class\"]:\n",
    "        \n",
    "            if len(item) > 1:\n",
    "                if item[1] == 'INC':\n",
    "                    copied_df.drop(copied_df[copied_df[\"class\"].isin([item])].index, inplace=True)\n",
    "                             \n",
    "                elif len(item) == 3:\n",
    "                    item[2] = 'INC'\n",
    "                    \n",
    "                else:\n",
    "                    item[1] = 'INC' \n",
    "                #print(item)\n",
    "        \n",
    "        copied_list = copied_df.fillna('none').to_dict('records')\n",
    "    \n",
    "\n",
    "        copied_labels = copied_df[\"class\"]\n",
    "        copied_labels1 = copied_labels.to_list()\n",
    "        copied_all_labels[i] = copied_labels1\n",
    "        copied_list = copied_df.drop(\"class\",axis=1)\n",
    "        copied_list1 = copied_list.fillna('none').to_dict('records')\n",
    "        \n",
    "        copied[i] = copied_list1 # put df for each incorrect particle in a dict\n",
    "        \n",
    "    return copied, copied_all_labels\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "all_prt = ['に','で', 'を','は','が','と','も','の','NULL','Other']\n",
    "\n",
    "\n",
    "prt = 'に'\n",
    "final_ni, final_labels_ni = copied_together(everything_ni,all_labels_ni,all_prt,prt)\n",
    "\n",
    "prt = 'で'\n",
    "final_de, final_labels_de = copied_together(everything_de,all_labels_de,all_prt,prt)\n",
    "\n",
    "prt = 'を'\n",
    "final_o, final_labels_o = copied_together(everything_o,all_labels_o,all_prt,prt)\n",
    "\n",
    "prt = 'は'\n",
    "final_wa, final_labels_wa = copied_together(everything_wa,all_labels_wa,all_prt,prt)\n",
    "\n",
    "prt = 'が'\n",
    "final_ga, final_labels_ga = copied_together(everything_ga,all_labels_ga,all_prt,prt)\n",
    "\n",
    "prt = 'と'\n",
    "final_to, final_labels_to = copied_together(everything_to,all_labels_to,all_prt,prt)\n",
    "\n",
    "prt = 'も'\n",
    "final_mo, final_labels_mo = copied_together(everything_mo,all_labels_mo,all_prt,prt)\n",
    "\n",
    "prt = 'の'\n",
    "final_no, final_labels_no = copied_together(everything_no,all_labels_no,all_prt,prt)\n",
    "\n",
    "#prt = 'Null'\n",
    "#final_null, final_labels_NULL = copied_together(everything_NULL,all_labels_NULL,all_prt,prt)\n",
    "\n",
    "#prt = 'Other'\n",
    "#final_other, final_labels_other = copied_together(everything_other,all_labels_other,all_prt,prt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687\n",
      "2641\n"
     ]
    }
   ],
   "source": [
    "prt='で'#change 'everything_*' and final_* below\n",
    "part = 'de' #used for file name \n",
    "part_cap = 'De' #used for file name\n",
    "\n",
    "\n",
    "train_set = everything_de+ final_mo[prt]+final_to[prt]+final_o[prt]+final_ga[prt]+final_ni[prt]+final_no[prt]+final_wa[prt]\n",
    "print(len(everything_de))\n",
    "print(len(train_set))\n",
    "\n",
    "train_labels = all_labels_de+final_labels_mo[prt]+final_labels_to[prt]+final_labels_o[prt]+final_labels_ga[prt]+final_labels_ni[prt]+final_labels_no[prt]+final_labels_wa[prt]\n",
    "\n",
    "new_train_labels = []\n",
    "for item in train_labels:\n",
    "    new_train_labels.append(str(item))\n",
    "train_labels=new_train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173 173\n"
     ]
    }
   ],
   "source": [
    "##dev\n",
    "\n",
    "path = '/home/mhiraga/CorpusData_New/Corpus_Data/experiment/dev_and_test/*'\n",
    "dev_everything, dev_all_labels = put_together_1(path,prt)    \n",
    "print(len(dev_everything), len(dev_all_labels))\n",
    "\n",
    "\n",
    "new_dev_all_labels = []\n",
    "for item in dev_all_labels:\n",
    "    new_dev_all_labels.append(str(item))\n",
    "    \n",
    "#print(new_dev_all_labels[:10])\n",
    "dev_all_labels=new_dev_all_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Class Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Loc', 'COR' & 374 & 14.16 \\\\\n",
      "'Time', 'INC' & 297 & 11.25 \\\\\n",
      "'Toward', 'INC' & 294 & 11.13 \\\\\n",
      "'MakeAdv', 'INC' & 253 & 9.58 \\\\\n",
      "'StillLoc', 'INC' & 245 & 9.28 \\\\\n",
      "'NiChange', 'INC' & 244 & 9.24 \\\\\n",
      "'NiID', 'INC' & 189 & 7.16 \\\\\n",
      "'IO', 'INC' & 144 & 5.45 \\\\\n",
      "'Instr', 'COR' & 71 & 2.69 \\\\\n",
      "'Agent', 'INC' & 68 & 2.57 \\\\\n",
      "'Unknown', 'NULL', 'INC' & 63 & 2.39 \\\\\n",
      "'Cause', 'COR' & 45 & 1.70 \\\\\n",
      "'Duration-Completion', 'COR' & 35 & 1.33 \\\\\n",
      "'Unknown', 'INC' & 35 & 1.33 \\\\\n",
      "'Surface', 'INC' & 33 & 1.25 \\\\\n",
      "'NiStinulus', 'INC' & 32 & 1.21 \\\\\n",
      "'Unknown', 'Other', 'INC' & 29 & 1.10 \\\\\n",
      "'NiReciprocal', 'INC' & 28 & 1.06 \\\\\n",
      "'ModeOfAgent', 'COR' & 27 & 1.02 \\\\\n",
      "'NiFor', 'INC' & 20 & 0.76 \\\\\n",
      "'Unknown', 'COR' & 19 & 0.72 \\\\\n",
      "'Appear/Manner', 'INC' & 19 & 0.72 \\\\\n",
      "'Lang', 'COR' & 15 & 0.57 \\\\\n",
      "'Unknown', 'De', 'INC' & 9 & 0.34 \\\\\n",
      "'TimeExpression', 'INC' & 7 & 0.27 \\\\\n",
      "'NiAbout', 'INC' & 6 & 0.23 \\\\\n",
      "'COR' & 5 & 0.19 \\\\\n",
      "'DO', 'INC' & 3 & 0.11 \\\\\n",
      "'Material', 'COR' & 3 & 0.11 \\\\\n",
      "'Attrib', 'INC' & 3 & 0.11 \\\\\n",
      "'OtherP', 'INC' & 2 & 0.08 \\\\\n",
      "'Ospace', 'INC' & 2 & 0.08 \\\\\n",
      "'Unknown', 'Ni', 'INC' & 2 & 0.08 \\\\\n",
      "'During', 'INC' & 2 & 0.08 \\\\\n",
      "'INC' & 2 & 0.08 \\\\\n",
      "'SubSub', 'Topic', 'INC' & 2 & 0.08 \\\\\n",
      "'Sub', 'INC' & 1 & 0.04 \\\\\n",
      "'AdjConj', 'INC' & 1 & 0.04 \\\\\n",
      "'And', 'INC' & 1 & 0.04 \\\\\n",
      "'Time', 'COR' & 1 & 0.04 \\\\\n",
      "'Unknown', 'To', 'INC' & 1 & 0.04 \\\\\n",
      "'Sound/Manner', 'INC' & 1 & 0.04 \\\\\n",
      "'VerbConj', 'INC' & 1 & 0.04 \\\\\n",
      "'NiChange', 'DO', 'INC' & 1 & 0.04 \\\\\n",
      "'Ability', 'INC' & 1 & 0.04 \\\\\n",
      "'For/About/Against', 'INC' & 1 & 0.04 \\\\\n",
      "'For', 'INC' & 1 & 0.04 \\\\\n",
      "'Loc', 'Topic', 'INC' & 1 & 0.04 \\\\\n",
      "'Agent', 'Topic', 'INC' & 1 & 0.04 \\\\\n",
      "'DO', 'Topic', 'INC' & 1 & 0.04 \\\\\n",
      "total: &  2641 \\\\\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "#print(list(le.classes_))\n",
    "counter=collections.Counter(train_labels)\n",
    "allcount = 0\n",
    "\n",
    "newcount={k: v for k, v in sorted(counter.items(), key=lambda item: item[1], reverse=True)}\n",
    "for label,count in newcount.items():\n",
    "    print(label[1:-1], '&', count, '&', \"%0.2f\" %(count*100/len(train_labels)), '\\\\\\\\')\n",
    "    allcount += count\n",
    "print('total: & ', allcount, '\\\\\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" #used for model selection\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\"\"\"\n",
    "####################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import SMOTE# , BorderlineSMOTE, SVMSMOTE, SMOTENC\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from imblearn.base import BaseSampler\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "def countLabel(list1):\n",
    "    set1 = set(list1)\n",
    "\n",
    "def fscoreToDF(fscore):\n",
    "    result = []\n",
    "    for x in fscore:\n",
    "        result.append(x)\n",
    "    fscoredf = pd.DataFrame(result, columns=reslist,index=['P','R','F','Total'])\n",
    "    #fscoredf.index=['label','P','R','F','Total']\n",
    "    #fscoredf.index.name = 'label'\n",
    "    return fscoredf\n",
    "    \n",
    "def labelCompare(list1, list2):\n",
    "    correctdict = {}\n",
    "    incorrectdict = {}\n",
    "    for count in range(0,len(list1)):\n",
    "        if list1[count] == list2[count]:\n",
    "            if list1[count] not in correctdict:\n",
    "                correctdict[list1[count]] = 1\n",
    "            else:\n",
    "                correctdict[list1[count]] = correctdict[list1[count]]+1\n",
    "        else:\n",
    "            if list1[count] not in incorrectdict:\n",
    "                incorrectdict[list1[count]]=1\n",
    "            else:\n",
    "                incorrectdict[list1[count]] = incorrectdict[list1[count]] +1\n",
    "    #print('Right')\n",
    "    outputlist = []\n",
    "    for key, value in correctdict.items():\n",
    "        output = {}\n",
    "        if key in incorrectdict:\n",
    "            inco_count = incorrectdict[key]\n",
    "        else:\n",
    "            inco_count = 0\n",
    "        total = value + inco_count\n",
    "        acc = value/total\n",
    "        #output['label'] = key\n",
    "        output['No.Right'] = value\n",
    "        output['No.Wrong'] = inco_count\n",
    "        output['Total'] = total\n",
    "        output['Accuracy'] = acc\n",
    "        output['label'] = str(key)\n",
    "        outputlist.append(output)\n",
    "        #print(key, ',', value, ',', inco_count, ',', total, ',', acc)\n",
    "            \n",
    "   \n",
    "    for key, value in incorrectdict.items(): \n",
    "        output={}\n",
    "        #print(key, ',', value)\n",
    "        if key not in correctdict:\n",
    "            co_count = 0\n",
    "            total = value + co_count\n",
    "            acc = co_count/total\n",
    "            #output['label'] = key\n",
    "            output['No.Right'] = co_count\n",
    "            output['No.Wrong'] = value\n",
    "            output['Total'] = total\n",
    "            output['Accuracy'] = acc\n",
    "            output['label'] = str(key)\n",
    "            outputlist.append(output)\n",
    "        else:\n",
    "            co_count = correctdict[key]\n",
    "            \n",
    "    res = pd.DataFrame(outputlist)\n",
    "    res = res[['label','Accuracy','Total','No.Right','No.Wrong']]\n",
    "    \n",
    "    return res\n",
    "\n",
    "def labelEncode(final_labels):      \n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    final_labels_encoded = []\n",
    "    for item in final_labels:\n",
    "        final_labels_encoded.append(str(item))\n",
    "    le.fit(final_labels_encoded)   \n",
    "    return final_labels_encoded, le.classes_\n",
    "\n",
    "    \n",
    "def print_features(v):\n",
    "    print(\"Number of Features: {} \\\\\\\\\".format(len(v.get_feature_names())))\n",
    "    features = v.get_feature_names()\n",
    "    feature_set = set()\n",
    "    for i in range(0,len(features)):\n",
    "        m = features[i].find(\"=\")\n",
    "        feature_set.add(features[i][0:m])\n",
    "    \n",
    "    print(feature_set)\n",
    "\n",
    "def print_prf(dev_array,predicted):\n",
    "    scores = precision_recall_fscore_support(dev_array,predicted, average='weighted')\n",
    "    scores = [n for n in scores]\n",
    "    for i in range(0,len(scores)-1):\n",
    "        if i == len(scores)-2:\n",
    "            print(scores[i]*100, '\\\\\\\\')\n",
    "        else:\n",
    "            print(scores[i]*100, '&', end=' ')\n",
    "\n",
    "def print_fscore(fscore_average): #tuple\n",
    "    fscore_list = list(fscore_average)\n",
    "    print(\"Total Weighted & \")\n",
    "    for i in range(len(fscore_list)-1):\n",
    "        new_score = (fscore_list[i])*100\n",
    "        print(\"%0.1f\" % new_score, end=\" &\")\n",
    "    print(\"\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 8231 \\\\\n",
      "Number of Features: 8231 \\\\\n",
      "{'word-2', 'Verbs_0', 'word+2', 'Verbs_4', 'OtherPrt_0', 'Verbs_7', 'lemma+1', 'OtherPrt_3', 'POS+2', 'POS-3', 'dep_head', 'Verbs_1', 'word-1', 'Verbs_2', 'Verbs_10', 'word-3', 'Verbs_6', 'POS+3', 'Verbs_9', 'POS-2', 'word+1', 'Verbs_3', 'Verbs_5', 'OtherPrt_2', 'POS-1', 'POS+1', 'Verbs_8', 'word+3', 'OtherPrt_1', 'lemma-1', 'target'}\n",
      "\n",
      "\n",
      "X_train shape: (2641, 8231) \\\\\n",
      "Logistic Regression \\\\\n",
      "X_dev shape: (173, 8231) \\\\\n",
      "Accuracy score: 0.653179190751445 \\\\\n",
      "number of labels: 19 \\\\\n",
      "number of each class: Counter({\"['Attrib', 'INC']\": 1, \"['Cause', 'COR']\": 1, \"['DO', 'INC']\": 1, \"['Duration-Completion', 'COR']\": 1, \"['Even', 'INC']\": 1, \"['Instr', 'COR']\": 1, \"['Lang', 'COR']\": 1, \"['Loc', 'COR']\": 1, \"['MakeAdv', 'INC']\": 1, \"['Material', 'COR']\": 1, \"['ModeOfAgent', 'COR']\": 1, \"['OtherP', 'INC']\": 1, \"['StillLoc', 'COR']\": 1, \"['StillLoc', 'INC']\": 1, \"['Surface', 'INC']\": 1, \"['Time', 'INC']\": 1, \"['Toward', 'INC']\": 1, \"['Unknown', 'COR']\": 1, \"['Unknown', 'NULL', 'INC']\": 1}) \\\\\n",
      "[\"['Attrib', 'INC']\", \"['Cause', 'COR']\", \"['DO', 'INC']\", \"['Duration-Completion', 'COR']\", \"['Even', 'INC']\", \"['Instr', 'COR']\", \"['Lang', 'COR']\", \"['Loc', 'COR']\", \"['MakeAdv', 'INC']\", \"['Material', 'COR']\", \"['ModeOfAgent', 'COR']\", \"['OtherP', 'INC']\", \"['StillLoc', 'COR']\", \"['StillLoc', 'INC']\", \"['Surface', 'INC']\", \"['Time', 'INC']\", \"['Toward', 'INC']\", \"['Unknown', 'COR']\", \"['Unknown', 'NULL', 'INC']\"]\n",
      "(array([0.        , 1.        , 0.        , 0.66666667, 0.        ,\n",
      "       0.375     , 0.        , 0.6462585 , 0.        , 0.        ,\n",
      "       0.85714286, 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        ]), array([0.        , 0.2       , 0.        , 0.5       , 0.        ,\n",
      "       0.33333333, 0.        , 0.97938144, 0.        , 0.        ,\n",
      "       0.6       , 0.        , 0.        , 0.16666667, 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        ]), array([0.        , 0.33333333, 0.        , 0.57142857, 0.        ,\n",
      "       0.35294118, 0.        , 0.77868852, 0.        , 0.        ,\n",
      "       0.70588235, 0.        , 0.        , 0.28571429, 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        ]), array([ 1, 15,  2,  8,  1,  9,  1, 97,  1,  1, 10,  1,  1, 12,  1,  2,  1,\n",
      "        8,  1]))\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "label &      P &     R &    F1 &  Total \\\\\n",
      "\\midrule\n",
      "['Loc', 'COR']                 &   64.6 &  97.9 &  77.9 &     97 \\\\\n",
      "['Cause', 'COR']               &  100.0 &  20.0 &  33.3 &     15 \\\\\n",
      "['StillLoc', 'INC']            &  100.0 &  16.7 &  28.6 &     12 \\\\\n",
      "['ModeOfAgent', 'COR']         &   85.7 &  60.0 &  70.6 &     10 \\\\\n",
      "['Instr', 'COR']               &   37.5 &  33.3 &  35.3 &      9 \\\\\n",
      "['Duration-Completion', 'COR'] &   66.7 &  50.0 &  57.1 &      8 \\\\\n",
      "['Unknown', 'COR']             &    0.0 &   0.0 &   0.0 &      8 \\\\\n",
      "['DO', 'INC']                  &    0.0 &   0.0 &   0.0 &      2 \\\\\n",
      "['Time', 'INC']                &    0.0 &   0.0 &   0.0 &      2 \\\\\n",
      "['Attrib', 'INC']              &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Toward', 'INC']              &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Surface', 'INC']             &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Material', 'COR']            &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['StillLoc', 'COR']            &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['OtherP', 'INC']              &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['MakeAdv', 'INC']             &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Lang', 'COR']                &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Even', 'INC']                &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Unknown', 'NULL', 'INC']     &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Total Weighted & \n",
      "61.8 &65.3 &57.1 &\n",
      "##########################\n",
      "random oversampling\n",
      "##########################\n",
      "(18700, 8231) \\\\\n",
      "Resampled dataset shape Counter({\"['Cause', 'COR']\": 374, \"['Loc', 'COR']\": 374, \"['Unknown', 'COR']\": 374, \"['DO', 'INC']\": 374, \"['TimeExpression', 'INC']\": 374, \"['Lang', 'COR']\": 374, \"['Instr', 'COR']\": 374, \"['ModeOfAgent', 'COR']\": 374, \"['Duration-Completion', 'COR']\": 374, \"['StillLoc', 'INC']\": 374, \"['Sub', 'INC']\": 374, \"['Time', 'INC']\": 374, \"['Surface', 'INC']\": 374, \"['OtherP', 'INC']\": 374, \"['Appear/Manner', 'INC']\": 374, \"['Material', 'COR']\": 374, \"['Ospace', 'INC']\": 374, \"['AdjConj', 'INC']\": 374, \"['Toward', 'INC']\": 374, \"['Unknown', 'Other', 'INC']\": 374, \"['NiID', 'INC']\": 374, \"['And', 'INC']\": 374, \"['Time', 'COR']\": 374, \"['COR']\": 374, \"['Unknown', 'NULL', 'INC']\": 374, \"['Attrib', 'INC']\": 374, \"['Unknown', 'To', 'INC']\": 374, \"['Sound/Manner', 'INC']\": 374, \"['VerbConj', 'INC']\": 374, \"['IO', 'INC']\": 374, \"['MakeAdv', 'INC']\": 374, \"['Unknown', 'De', 'INC']\": 374, \"['Unknown', 'Ni', 'INC']\": 374, \"['NiChange', 'DO', 'INC']\": 374, \"['Agent', 'INC']\": 374, \"['NiChange', 'INC']\": 374, \"['NiReciprocal', 'INC']\": 374, \"['NiStinulus', 'INC']\": 374, \"['During', 'INC']\": 374, \"['Unknown', 'INC']\": 374, \"['NiFor', 'INC']\": 374, \"['NiAbout', 'INC']\": 374, \"['Ability', 'INC']\": 374, \"['For/About/Against', 'INC']\": 374, \"['For', 'INC']\": 374, \"['INC']\": 374, \"['Loc', 'Topic', 'INC']\": 374, \"['SubSub', 'Topic', 'INC']\": 374, \"['Agent', 'Topic', 'INC']\": 374, \"['DO', 'Topic', 'INC']\": 374}) \\\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhiraga/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mhiraga/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mhiraga/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: RandomOverSampling \\\\\n",
      "Accuracy score: 0.6763005780346821 \\\\\n",
      "number of labels: 19 \\\\\n",
      "\n",
      "\n",
      "\n",
      "########################Oversampling Table#####################\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "label &      P &     R &    F1 &  Total \\\\\n",
      "\\midrule\n",
      "['Attrib', 'INC']              &    0.0 &   0.0 &   0.0 &    1.0 \\\\\n",
      "['Cause', 'COR']               &  100.0 &  33.3 &  50.0 &   15.0 \\\\\n",
      "['DO', 'INC']                  &    0.0 &   0.0 &   0.0 &    2.0 \\\\\n",
      "['Duration-Completion', 'COR'] &   62.5 &  62.5 &  62.5 &    8.0 \\\\\n",
      "['Even', 'INC']                &    0.0 &   0.0 &   0.0 &    1.0 \\\\\n",
      "['Instr', 'COR']               &   33.3 &  44.4 &  38.1 &    9.0 \\\\\n",
      "['Lang', 'COR']                &    0.0 &   0.0 &   0.0 &    1.0 \\\\\n",
      "['Loc', 'COR']                 &   69.6 &  96.9 &  81.0 &   97.0 \\\\\n",
      "['MakeAdv', 'INC']             &    0.0 &   0.0 &   0.0 &    1.0 \\\\\n",
      "['Material', 'COR']            &    0.0 &   0.0 &   0.0 &    1.0 \\\\\n",
      "['ModeOfAgent', 'COR']         &   87.5 &  70.0 &  77.8 &   10.0 \\\\\n",
      "['OtherP', 'INC']              &    0.0 &   0.0 &   0.0 &    1.0 \\\\\n",
      "['StillLoc', 'COR']            &    0.0 &   0.0 &   0.0 &    1.0 \\\\\n",
      "['StillLoc', 'INC']            &  100.0 &  16.7 &  28.6 &   12.0 \\\\\n",
      "['Surface', 'INC']             &    0.0 &   0.0 &   0.0 &    1.0 \\\\\n",
      "['Time', 'INC']                &    0.0 &   0.0 &   0.0 &    2.0 \\\\\n",
      "['Toward', 'INC']              &    0.0 &   0.0 &   0.0 &    1.0 \\\\\n",
      "['Unknown', 'COR']             &    0.0 &   0.0 &   0.0 &    8.0 \\\\\n",
      "['Unknown', 'NULL', 'INC']     &    0.0 &   0.0 &   0.0 &    1.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Total Weighted & \n",
      "64.3 &67.6 &61.1 &\n",
      "#####################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhiraga/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mhiraga/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mhiraga/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "v = DictVectorizer()\n",
    "\n",
    "\n",
    "X_train = v.fit_transform(train_set)\n",
    "\n",
    "print(\"Number of Features: {} \\\\\\\\\".format(len(v.get_feature_names())))\n",
    "print_features(v)\n",
    "\n",
    "\n",
    "X_dev = v.transform(dev_everything)\n",
    "print('\\n')\n",
    "\n",
    "clf = LogisticRegression(multi_class='ovr',max_iter=2000)\n",
    "print('X_train shape: {} \\\\\\\\'.format(X_train.shape))\n",
    "\n",
    "\n",
    "#y_train = MultiLabelBinarizer().fit_transform(final_labels_ni)\n",
    "y_train = labelEncode(train_labels)[0]\n",
    "clf.fit(X_train,y_train)\n",
    "predicted = clf.predict(X_dev)\n",
    "\n",
    "\n",
    "\n",
    "dev_array = np.asarray(dev_all_labels)\n",
    "\n",
    "\n",
    "y_dev = labelEncode(dev_all_labels)[0]\n",
    "print('Logistic Regression \\\\\\\\')\n",
    "print('X_dev shape: {} \\\\\\\\'.format(X_dev.shape))\n",
    "print('Accuracy score: {} \\\\\\\\'.format(clf.score(X_dev,y_dev)))\n",
    "\n",
    "\n",
    "res = labelCompare(y_dev, predicted)\n",
    "\n",
    "res.to_csv(path_or_buf=\"/home/mhiraga/CorpusData_New/Corpus_Data/multiclass_results/Ni_AllResults_latex/Generated_LogReg_ni\",sep='&')\n",
    "\n",
    "\n",
    "\n",
    "reslist = list(labelEncode(dev_all_labels)[1])\n",
    "\n",
    "print('number of labels: {} \\\\\\\\'.format(len(reslist)))\n",
    "print('number of each class: {} \\\\\\\\'.format(Counter(reslist)))\n",
    "print(reslist)\n",
    "#F1 score\n",
    "fscore = precision_recall_fscore_support(dev_array,predicted,labels=reslist)\n",
    "print(fscore)\n",
    "#F0.5 score\n",
    "fscore_point5 = precision_recall_fscore_support(dev_array,predicted,labels=reslist,beta=0.5)\n",
    "\n",
    "# put them in df\n",
    "fscoredf = fscoreToDF(fscore)\n",
    "fscoredf = fscoredf.round(3).transpose()\n",
    "fscoredf.columns = ['P','R','F1','Total']\n",
    "fscoredf= fscoredf.rename_axis('label', axis=1)\n",
    "fscoredf[['P','R','F1']] = fscoredf[['P','R','F1']].multiply(100)\n",
    "fscoredf[['Total']] = fscoredf[['Total']].astype(int)\n",
    "fscoredf = fscoredf.sort_values(by='Total',ascending=False)\n",
    "\n",
    "print(fscoredf.to_latex())\n",
    "fscore_average_ovs = precision_recall_fscore_support(dev_array,predicted,average='weighted')\n",
    "print_fscore(fscore_average_ovs)\n",
    "\n",
    "print('##########################')\n",
    "print('random oversampling')\n",
    "print('##########################')\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_resample(X_train, train_labels)\n",
    "print(X_res.shape, \"\\\\\\\\\")\n",
    "print('Resampled dataset shape %s \\\\\\\\' % Counter(y_res))\n",
    "\n",
    "pipe = make_pipeline(RandomOverSampler(random_state=42), LogisticRegression(multi_class='ovr',max_iter=2000))\n",
    "pipe.fit(X_train, train_labels)\n",
    "predicted_1 = pipe.predict(X_dev)\n",
    "\n",
    "\n",
    "dev_array = np.asarray(dev_all_labels)\n",
    "#print(dev_array)\n",
    "print('LR: RandomOverSampling \\\\\\\\')\n",
    "res_ov = labelCompare(dev_array, predicted_1)\n",
    "print('Accuracy score: {} \\\\\\\\'.format(pipe.score(X_dev,dev_array)))\n",
    "print('number of labels: {} \\\\\\\\'.format(len(reslist)))\n",
    "\n",
    "fscore_ov = precision_recall_fscore_support(dev_array,predicted_1,labels=reslist)\n",
    "fscoredf_ov = fscoreToDF(fscore_ov)\n",
    "fscoredf_ov = fscoredf_ov.round(3).transpose()\n",
    "fscoredf_ov.columns = ['P','R','F1','Total']\n",
    "fscoredf_ov[['P','R','F1']] = fscoredf_ov[['P','R','F1']]*100\n",
    "fscoredf_ov= fscoredf_ov.rename_axis('label', axis=1)\n",
    "fscoredf[['Total']] = fscoredf[['Total']].astype(int)\n",
    "fscoredf = fscoredf.sort_values(by='Total',ascending=False)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"########################Oversampling Table#####################\")\n",
    "print(fscoredf_ov.to_latex())\n",
    "fscore_average_ovs = precision_recall_fscore_support(dev_array,predicted_1,average='weighted')\n",
    "print_fscore(fscore_average_ovs)\n",
    "\n",
    "print('#####################\\n\\n')\n",
    "#fit(X_train, all_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############FINAL TABLE################################\n",
      "label                               P     R    F1      P     R    F1  Total\n",
      "['Loc', 'COR']                   64.6  97.9  77.9   69.6  96.9  81.0     97\n",
      "['Cause', 'COR']                100.0  20.0  33.3  100.0  33.3  50.0     15\n",
      "['StillLoc', 'INC']             100.0  16.7  28.6  100.0  16.7  28.6     12\n",
      "['ModeOfAgent', 'COR']           85.7  60.0  70.6   87.5  70.0  77.8     10\n",
      "['Instr', 'COR']                 37.5  33.3  35.3   33.3  44.4  38.1      9\n",
      "['Duration-Completion', 'COR']   66.7  50.0  57.1   62.5  62.5  62.5      8\n",
      "['Unknown', 'COR']                0.0   0.0   0.0    0.0   0.0   0.0      8\n",
      "['DO', 'INC']                     0.0   0.0   0.0    0.0   0.0   0.0      2\n",
      "['Time', 'INC']                   0.0   0.0   0.0    0.0   0.0   0.0      2\n",
      "['StillLoc', 'COR']               0.0   0.0   0.0    0.0   0.0   0.0      1\n",
      "['Toward', 'INC']                 0.0   0.0   0.0    0.0   0.0   0.0      1\n",
      "['Surface', 'INC']                0.0   0.0   0.0    0.0   0.0   0.0      1\n",
      "['Material', 'COR']               0.0   0.0   0.0    0.0   0.0   0.0      1\n",
      "['OtherP', 'INC']                 0.0   0.0   0.0    0.0   0.0   0.0      1\n",
      "['Attrib', 'INC']                 0.0   0.0   0.0    0.0   0.0   0.0      1\n",
      "['MakeAdv', 'INC']                0.0   0.0   0.0    0.0   0.0   0.0      1\n",
      "['Lang', 'COR']                   0.0   0.0   0.0    0.0   0.0   0.0      1\n",
      "['Even', 'INC']                   0.0   0.0   0.0    0.0   0.0   0.0      1\n",
      "['Unknown', 'NULL', 'INC']        0.0   0.0   0.0    0.0   0.0   0.0      1\n",
      "\n",
      "\n",
      " Latex table\n",
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      "label &      P &     R &    F1 &      P &     R &    F1 &  Total \\\\\n",
      "\\midrule\n",
      "['Loc', 'COR']                 &   64.6 &  97.9 &  77.9 &   69.6 &  96.9 &  81.0 &     97 \\\\\n",
      "['Cause', 'COR']               &  100.0 &  20.0 &  33.3 &  100.0 &  33.3 &  50.0 &     15 \\\\\n",
      "['StillLoc', 'INC']            &  100.0 &  16.7 &  28.6 &  100.0 &  16.7 &  28.6 &     12 \\\\\n",
      "['ModeOfAgent', 'COR']         &   85.7 &  60.0 &  70.6 &   87.5 &  70.0 &  77.8 &     10 \\\\\n",
      "['Instr', 'COR']               &   37.5 &  33.3 &  35.3 &   33.3 &  44.4 &  38.1 &      9 \\\\\n",
      "['Duration-Completion', 'COR'] &   66.7 &  50.0 &  57.1 &   62.5 &  62.5 &  62.5 &      8 \\\\\n",
      "['Unknown', 'COR']             &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      8 \\\\\n",
      "['DO', 'INC']                  &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      2 \\\\\n",
      "['Time', 'INC']                &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      2 \\\\\n",
      "['StillLoc', 'COR']            &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Toward', 'INC']              &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Surface', 'INC']             &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Material', 'COR']            &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['OtherP', 'INC']              &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Attrib', 'INC']              &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['MakeAdv', 'INC']             &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Lang', 'COR']                &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Even', 'INC']                &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Unknown', 'NULL', 'INC']     &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "#####################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### merge two dfs f1\n",
    "###########################\n",
    "\n",
    "\n",
    "fscoredf = fscoredf.drop(['Total'],axis=1)\n",
    "\n",
    "frame = [fscoredf, fscoredf_ov]\n",
    "\n",
    "result_ov = pd.concat(frame,axis=1)\n",
    "\n",
    "result_ov['Total'] = result_ov['Total'].astype(int)\n",
    "\n",
    "result_ov = result_ov.sort_values(by='Total',ascending=False)\n",
    "\n",
    "print('##############FINAL TABLE################################')\n",
    "print(result_ov)\n",
    "print('\\n\\n Latex table')\n",
    "print(result_ov.to_latex())\n",
    "print('#####################\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "######################\n",
      "\n",
      "Synthetic Data w/o over sampling vs w/ oversampling\n",
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      "label &      P &     R &    F1 &      P &     R &    F1 &  Total \\\\\n",
      "\\midrule\n",
      "['Loc', 'COR']                 &   64.6 &  97.9 &  77.9 &   69.6 &  96.9 &  81.0 &     97 \\\\\n",
      "['Cause', 'COR']               &  100.0 &  20.0 &  33.3 &  100.0 &  33.3 &  50.0 &     15 \\\\\n",
      "['StillLoc', 'INC']            &  100.0 &  16.7 &  28.6 &  100.0 &  16.7 &  28.6 &     12 \\\\\n",
      "['ModeOfAgent', 'COR']         &   85.7 &  60.0 &  70.6 &   87.5 &  70.0 &  77.8 &     10 \\\\\n",
      "['Instr', 'COR']               &   37.5 &  33.3 &  35.3 &   33.3 &  44.4 &  38.1 &      9 \\\\\n",
      "['Duration-Completion', 'COR'] &   66.7 &  50.0 &  57.1 &   62.5 &  62.5 &  62.5 &      8 \\\\\n",
      "['Unknown', 'COR']             &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      8 \\\\\n",
      "['DO', 'INC']                  &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      2 \\\\\n",
      "['Time', 'INC']                &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      2 \\\\\n",
      "['StillLoc', 'COR']            &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Toward', 'INC']              &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Surface', 'INC']             &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Material', 'COR']            &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['OtherP', 'INC']              &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Attrib', 'INC']              &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['MakeAdv', 'INC']             &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Lang', 'COR']                &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Even', 'INC']                &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "['Unknown', 'NULL', 'INC']     &    0.0 &   0.0 &   0.0 &    0.0 &   0.0 &   0.0 &      1 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\\n######################\\n\")\n",
    "print(\"Synthetic Data w/o over sampling vs w/ oversampling\")\n",
    "#result_both.index = result_both.index.map(lambda x: str(x))\n",
    "result_ov.index = result_ov.index.map(lambda x: str(x))\n",
    "\n",
    "\n",
    "print(result_ov.to_latex())\n",
    "output_path = '/home/mhiraga/CorpusData_New/Corpus_Data/multiclass_results/{}_AllResults_latex/RandomOverSampling_{}'.format(part_cap, part)\n",
    "result_ov.to_csv(path_or_buf= output_path,sep='&')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Unnamed: 0      P     R    F1    P.1   R.1  F1.1\n",
      "0                  ['Loc', 'COR']   68.3  97.9  80.5   72.8  93.8  82.0\n",
      "1          ['N', '/', 'A', 'INC']   62.5  21.7  32.3   50.0  30.4  37.8\n",
      "2                ['Cause', 'COR']  100.0  26.7  42.1   80.0  26.7  40.0\n",
      "3          ['ModeOfAgent', 'COR']   87.5  70.0  77.8   87.5  70.0  77.8\n",
      "4                ['Instr', 'COR']   37.5  33.3  35.3   27.3  33.3  30.0\n",
      "5  ['Duration-Completion', 'COR']   66.7  50.0  57.1   66.7  75.0  70.6\n",
      "6              ['Unknown', 'COR']    0.0   0.0   0.0  100.0  12.5  22.2\n",
      "7             ['StillLoc', 'COR']    0.0   0.0   0.0    0.0   0.0   0.0\n",
      "8             ['Material', 'COR']    0.0   0.0   0.0    0.0   0.0   0.0\n",
      "9                 ['Lang', 'COR']    0.0   0.0   0.0    0.0   0.0   0.0\n",
      "                        Unnamed: 0      P     R    F1    P.1   R.1  F1.1  \\\n",
      "0                   ['Loc', 'COR']   64.6  97.9  77.9   69.6  96.9  81.0   \n",
      "1                 ['Cause', 'COR']  100.0  20.0  33.3  100.0  33.3  50.0   \n",
      "2              ['StillLoc', 'INC']  100.0  16.7  28.6  100.0  16.7  28.6   \n",
      "3           ['ModeOfAgent', 'COR']   85.7  60.0  70.6   87.5  70.0  77.8   \n",
      "4                 ['Instr', 'COR']   37.5  33.3  35.3   33.3  44.4  38.1   \n",
      "5   ['Duration-Completion', 'COR']   66.7  50.0  57.1   62.5  62.5  62.5   \n",
      "6               ['Unknown', 'COR']    0.0   0.0   0.0    0.0   0.0   0.0   \n",
      "7                    ['DO', 'INC']    0.0   0.0   0.0    0.0   0.0   0.0   \n",
      "8                  ['Time', 'INC']    0.0   0.0   0.0    0.0   0.0   0.0   \n",
      "9              ['StillLoc', 'COR']    0.0   0.0   0.0    0.0   0.0   0.0   \n",
      "10               ['Toward', 'INC']    0.0   0.0   0.0    0.0   0.0   0.0   \n",
      "11              ['Surface', 'INC']    0.0   0.0   0.0    0.0   0.0   0.0   \n",
      "12             ['Material', 'COR']    0.0   0.0   0.0    0.0   0.0   0.0   \n",
      "13               ['OtherP', 'INC']    0.0   0.0   0.0    0.0   0.0   0.0   \n",
      "14               ['Attrib', 'INC']    0.0   0.0   0.0    0.0   0.0   0.0   \n",
      "15              ['MakeAdv', 'INC']    0.0   0.0   0.0    0.0   0.0   0.0   \n",
      "16                 ['Lang', 'COR']    0.0   0.0   0.0    0.0   0.0   0.0   \n",
      "17                 ['Even', 'INC']    0.0   0.0   0.0    0.0   0.0   0.0   \n",
      "18      ['Unknown', 'NULL', 'INC']    0.0   0.0   0.0    0.0   0.0   0.0   \n",
      "\n",
      "    Total  \n",
      "0      97  \n",
      "1      15  \n",
      "2      12  \n",
      "3      10  \n",
      "4       9  \n",
      "5       8  \n",
      "6       8  \n",
      "7       2  \n",
      "8       2  \n",
      "9       1  \n",
      "10      1  \n",
      "11      1  \n",
      "12      1  \n",
      "13      1  \n",
      "14      1  \n",
      "15      1  \n",
      "16      1  \n",
      "17      1  \n",
      "18      1  \n",
      "\\begin{tabular}{lrrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} &    P\\_x &   R\\_x &  F1\\_x &  P.1\\_x &  R.1\\_x &  F1.1\\_x &    P\\_y &   R\\_y &  F1\\_y &  P.1\\_y &  R.1\\_y &  F1.1\\_y &  Total \\\\\n",
      "Unnamed: 0                     &        &       &       &        &        &         &        &       &       &        &        &         &        \\\\\n",
      "\\midrule\n",
      "['Loc', 'COR']                 &   68.3 &  97.9 &  80.5 &   72.8 &   93.8 &    82.0 &   64.6 &  97.9 &  77.9 &   69.6 &   96.9 &    81.0 &     97 \\\\\n",
      "['Cause', 'COR']               &  100.0 &  26.7 &  42.1 &   80.0 &   26.7 &    40.0 &  100.0 &  20.0 &  33.3 &  100.0 &   33.3 &    50.0 &     15 \\\\\n",
      "['ModeOfAgent', 'COR']         &   87.5 &  70.0 &  77.8 &   87.5 &   70.0 &    77.8 &   85.7 &  60.0 &  70.6 &   87.5 &   70.0 &    77.8 &     10 \\\\\n",
      "['Instr', 'COR']               &   37.5 &  33.3 &  35.3 &   27.3 &   33.3 &    30.0 &   37.5 &  33.3 &  35.3 &   33.3 &   44.4 &    38.1 &      9 \\\\\n",
      "['Duration-Completion', 'COR'] &   66.7 &  50.0 &  57.1 &   66.7 &   75.0 &    70.6 &   66.7 &  50.0 &  57.1 &   62.5 &   62.5 &    62.5 &      8 \\\\\n",
      "['Unknown', 'COR']             &    0.0 &   0.0 &   0.0 &  100.0 &   12.5 &    22.2 &    0.0 &   0.0 &   0.0 &    0.0 &    0.0 &     0.0 &      8 \\\\\n",
      "['StillLoc', 'COR']            &    0.0 &   0.0 &   0.0 &    0.0 &    0.0 &     0.0 &    0.0 &   0.0 &   0.0 &    0.0 &    0.0 &     0.0 &      1 \\\\\n",
      "['Material', 'COR']            &    0.0 &   0.0 &   0.0 &    0.0 &    0.0 &     0.0 &    0.0 &   0.0 &   0.0 &    0.0 &    0.0 &     0.0 &      1 \\\\\n",
      "['Lang', 'COR']                &    0.0 &   0.0 &   0.0 &    0.0 &    0.0 &     0.0 &    0.0 &   0.0 &   0.0 &    0.0 &    0.0 &     0.0 &      1 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## load file from Classification experiment and merge\n",
    "baseline_path = '/home/mhiraga/CorpusData_New/Corpus_Data/multiclass_results/{}_AllResults_latex/BaselineOversampling_{}'.format(part_cap, part)\n",
    "baseline = pd.read_csv(baseline_path ,sep='&')\n",
    "print(baseline)\n",
    "\n",
    "generated_path = '/home/mhiraga/CorpusData_New/Corpus_Data/multiclass_results/{}_AllResults_latex/RandomOverSampling_{}'.format(part_cap, part)\n",
    "generated_oversample = pd.read_csv(generated_path,sep='&')\n",
    "print(generated_oversample)\n",
    "\n",
    "all_together = baseline.merge(generated_oversample, how = 'inner', on = 'Unnamed: 0')\n",
    "all_together['Total'] = all_together['Total'].fillna(0.0).astype(int)\n",
    "all_together = all_together.set_index('Unnamed: 0')\n",
    "print(all_together.to_latex())\n",
    "\n",
    "all_together_output = '/home/mhiraga/CorpusData_New/Corpus_Data/multiclass_results/{}_AllResults_latex/AllTogetherLatex_{}'.format(part_cap, part)\n",
    "all_together.to_csv(path_or_buf=all_together_output,sep='&')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
